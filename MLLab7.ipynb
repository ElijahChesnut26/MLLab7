{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eli Chesnut and Kiersten Wener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/datatattle/covid-19-nlp-text-classification?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.38M/4.38M [00:02<00:00, 1.56MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /Users/kierstenwener/.cache/kagglehub/datasets/datatattle/covid-19-nlp-text-classification/versions/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"datatattle/covid-19-nlp-text-classification\")\n",
    "csv_train_file_path = os.path.join(path, \"Corona_NLP_train.csv\")\n",
    "csv_test_file_path = os.path.join(path, \"Corona_NLP_test.csv\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "\n",
    "# file = f\"{path}/Corona_NLP_train.csv\"\n",
    "# df = pd.read_csv(file)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(csv_train_file_path, encoding='ISO-8859-1') #can infer col names\n",
    "df_test = pd.read_csv(csv_test_file_path, encoding='ISO-8859-1')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment\n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative\n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive\n",
       "2  Find out how you can protect yourself and love...  Extremely Positive\n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative\n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = [\"UserName\", \"ScreenName\", \"Location\", \"TweetAt\"]\n",
    "df_train = df_train.drop(columns=columns_to_drop)\n",
    "df_test = df_test.drop(columns=columns_to_drop)\n",
    "df = pd.concat([df_test, df_train], ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355\n",
      "Crude oil dropped to its lowest in 17 years in Asia as #coronavirus in #US and #EU shows little sign of slowing.\n",
      "\n",
      "Noon prices:\n",
      "#SPX500Â2504.3\n",
      "#NAS100Â7507.1\n",
      "#WTIÂ20.24\n",
      "#GOLDÂ1618.08\n",
      "#SILVERÂ14.118\n",
      "\n",
      "Watch these prices closely: https://t.co/WuiEEfSNOj\n",
      "\n",
      "#COVID19 #CoronaVirusOutbreak https://t.co/j68MZIDXAd https://t.co/9MyqGxutUU\n"
     ]
    }
   ],
   "source": [
    "df[\"tweet_length\"] = df[\"OriginalTweet\"].apply(len)  # Add a column for tweet lengths\n",
    "max_length = df[\"tweet_length\"].max()         # Get the maximum length\n",
    "longest_tweet = df.loc[df[\"tweet_length\"] == max_length, \"OriginalTweet\"].iloc[0]  # Get the longest tweet\n",
    "\n",
    "print(max_length)\n",
    "print(longest_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this quick test to determine what the longest tweet is we can now set our max review length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment\n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative\n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive\n",
       "2  Find out how you can protect yourself and love...  Extremely Positive\n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative\n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_review_length = 400\n",
    "df = df.drop(columns=\"tweet_length\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "num_duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "#no duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44955 entries, 0 to 44954\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   OriginalTweet  44955 non-null  object\n",
      " 1   Sentiment      44955 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 702.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Extremely Negative' 'Positive' 'Extremely Positive' 'Negative' 'Neutral']\n"
     ]
    }
   ],
   "source": [
    "u = df[\"Sentiment\"].unique()\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Positive' 'Neutral']\n"
     ]
    }
   ],
   "source": [
    "df[\"Sentiment\"] = df[\"Sentiment\"].apply(lambda x: \"Negative\" if x == \"Extremely Negative\" else \"Positive\" if x == \"Extremely Positive\" else x)\n",
    "\n",
    "u = df[\"Sentiment\"].unique()\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment_Negative</th>\n",
       "      <th>Sentiment_Neutral</th>\n",
       "      <th>Sentiment_Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet  Sentiment_Negative  \\\n",
       "0  TRENDING: New Yorkers encounter empty supermar...                True   \n",
       "1  When I couldn't find hand sanitizer at Fred Me...               False   \n",
       "2  Find out how you can protect yourself and love...               False   \n",
       "3  #Panic buying hits #NewYork City as anxious sh...                True   \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...               False   \n",
       "\n",
       "   Sentiment_Neutral  Sentiment_Positive  \n",
       "0              False               False  \n",
       "1              False                True  \n",
       "2              False                True  \n",
       "3              False               False  \n",
       "4               True               False  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ohe = pd.get_dummies(df, columns=[\"Sentiment\"])\n",
    "df_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "targets = [\"Sentiment_Negative\",\"Sentiment_Neutral\", \"Sentiment_Positive\"]\n",
    "X = df_ohe.drop(columns=targets)\n",
    "y = df_ohe[targets]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setuptools.dist\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)  # Fit the tokenizer on the training data\n",
    "\n",
    "# Convert the text to sequences of integers\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to ensure uniform length\n",
    "max_review_length = 400\n",
    "X_train_padded = sequence.pad_sequences(X_train_sequences, maxlen=max_review_length)\n",
    "X_test_padded = sequence.pad_sequences(X_test_sequences, maxlen=max_review_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kierstenwener/Desktop/MLLab7/.venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Input\n",
    "\n",
    "EMBED_SIZE = 50\n",
    "# the input is a list of integers, 500 long\n",
    "sequence_input = Input(shape=(X_train.shape[1], ))\n",
    "\n",
    "embedded_sequences = Embedding(400, # input dimension (max int of OHE)\n",
    "                EMBED_SIZE, # output dimension size\n",
    "                input_length=max_review_length)(sequence_input) # number of words in each sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created the training and testing sets for our multi class classification. The dataset that we are using has several columns that are not relevant for this lab and we disgard them. The data is already split up into a training and testing CSV, but for the purposes of doing our own splitting these were combined into one dataframe. We then edited the targets to simplify the classification before one hot encoding them. Before padding the sentences we used the keras tokenizer giving us a word index. We then used Embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score is a particularly useful metric for classifying Covid-19 tweet sentiment into negative, positive, and neutral categories due to its ability to balance both precision and recall. In sentiment analysis tasks, especially with imbalanced datasets, the F1 score ensures that the model doesn't simply favor the majority class at the expense of identifying less frequent but equally important categories. Since misclassifying a sentiment can have significant implications, the F1 score helps optimize the trade-off between precision (avoiding false positives) and recall (avoiding false negatives), providing a more comprehensive evaluation of the model's performance across all sentiment classes. By focusing on both false positives and false negatives, the F1 score ensures that the model is robust and effective at correctly predicting sentiments for all classes, making it ideal for real-world applications such as analyzing public opinion during a crisis like the Covid-19 pandemic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Train-test split is a solid way to divide up a dataset because it gives a simple yet effective way to evaluate how well our model generalizes to new, unseen data. By splitting the data into two sets ensures that our model doesn't just memorize the data and reduces overfitting. It’s especially helpful for tasks like classifying Covid-19 tweet sentiments, where the tweets might vary widely in content and sentiment. Using train_test_split with shuffling helps mix things up, so we are not training on a block of tweets with one dominant sentiment and testing on another block with a different sentiment. This way we get a better picture of how our model will perform across all types of sentiments. This method also gives us an unbiased estimate of how well the model will do in real-world situations, where the data distribution is unpredictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate\n",
    "from tensorflow.keras.layers import Subtract\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "\n",
    "EMBED_SIZE = 50\n",
    "# the input is a list of integers, 500 long\n",
    "sequence_input = Input(shape=(X_train.shape[1], ))\n",
    "\n",
    "# this will reduce the input dimension from VOCAB_SIZE to 50 for each word\n",
    "# the lenght will be the maximum number of words in a document, so 500\n",
    "embedded_sequences = Embedding(top_words, # input dimension (max int of OHE)\n",
    "                EMBED_SIZE, # output dimension size\n",
    "                input_length=max_review_length)(sequence_input) # number of words in each sequence\n",
    " \n",
    "# starting sequence size is 500 (words) by 50 (embedded features)\n",
    "x = Conv1D(64, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(embedded_sequences)\n",
    "\n",
    "# after conv, length becomes: 500-4=496 \n",
    "# so overall size is 496 by 64\n",
    "\n",
    "# now pool across time\n",
    "x = MaxPooling1D(5)(x)# after max pool, 496/5 -> 99 by 64\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# extract additional features\n",
    "x = Conv1D(64, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "\n",
    "# new size is 95 after the conovlutions\n",
    "x = MaxPooling1D(5)(x) # after max pool, size is 95/5 = 19 by 64\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# more features through CNN processing!\n",
    "x = Conv1D(64, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# after convolution, size becomes 15 elements long\n",
    "# Take the mean of these elements across features, result is 64 elements\n",
    "x_mean = GlobalAveragePooling1D()(x) # this is the size to globally flatten \n",
    "\n",
    "# Take the variance of these elements across features, result is 64 elements\n",
    "x_tmp = Subtract()([x,x_mean])\n",
    "x_std = GlobalAveragePooling1D()(x_tmp**2)\n",
    "\n",
    "x = Concatenate(name='concat_1')([x_mean,x_std])\n",
    "\n",
    "x = Dense(64, activation='relu',\n",
    "          kernel_initializer='he_uniform')(x)\n",
    "\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "preds = Dense(NUM_CLASSES, activation='sigmoid',\n",
    "              kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# you will need to install pydot properly on your machine to get this running\n",
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
